{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# from nltk.tag.perceptron import PerceptronTagger\n",
    "# \n",
    "# corpus = postagging.eat_corpus(postagging.CORPUS)\n",
    "# k = round(len(corpus) * 0.8)\n",
    "# train_corpus = corpus[0:k]\n",
    "# test_corpus = corpus[k:]\n",
    "# perc_tagger = PerceptronTagger(load=False)\n",
    "# perc_tagger.train(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# n = 0\n",
    "# right = 0\n",
    "# for sentence in test_corpus:\n",
    "#     x = [word[0] for word in sentence]\n",
    "#     y = [word[1] for word in sentence]\n",
    "#     y_pred = [word[1] for word in perc_tagger.tag(x)]\n",
    "#     n += len(sentence)\n",
    "#     for tag_real, tag_pred in zip(y, y_pred):\n",
    "#         if tag_real == tag_pred:\n",
    "#             right += 1\n",
    "# right / n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "# \n",
    "# hmm_tagger = HiddenMarkovModelTagger.train(train_corpus, test_sequence=test_corpus)\n",
    "# "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# for sentence in test_corpus:\n",
    "#     x = [word[0] for word in sentence]\n",
    "#     y = [word[1] for word in sentence]\n",
    "#     y_pred = [word[1] for word in hmm_tagger.tag(x)]\n",
    "#     n += len(sentence)\n",
    "#     for tag_real, tag_pred in zip(y, y_pred):\n",
    "#         if tag_real == tag_pred:\n",
    "#             right += 1\n",
    "# right / n\n",
    "# "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# dictionary = postagging.eat_dictionary(postagging.DICTIONARY)\n",
    "# "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# text = []\n",
    "# with open('dataset_37845_1.txt', encoding='UTF-8') as file:\n",
    "#     for line in file.readlines():\n",
    "#         text.append(postagging.prepare_sentence(line))\n",
    "# \n",
    "# with open('output', 'w', encoding='UTF-8') as file:\n",
    "#     for sentence in text:\n",
    "#         res = hmm_tagger.tag([word.lower() for word in sentence])\n",
    "#         output = []\n",
    "#         for word, tag in zip(sentence, res):\n",
    "#             if '_'.join(tag) in dictionary:\n",
    "#                 lemma = dictionary['_'.join(tag)]\n",
    "#             else:\n",
    "#                 lemma = tag[0]\n",
    "#             output.append(word + '{' + lemma + '=' + tag[1] + '}')\n",
    "#         file.write(' '.join(output) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# M = np.array([[0, 1, 0, 1, 1],[1, 1, 1, 1, 1],[0, 1, 0, 0, 0],[0, 0, 1, 0, 1],[1, 0, 0, 0, 0]], dtype=float)\n",
    "# ones = np.ones((5,5))\n",
    "# MM = M.dot(ones) * M"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# f = lambda x: 1 / x\n",
    "# MM = f(MM) * M\n",
    "# MM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# where = np.isnan(MM)\n",
    "# MM[where] = 0\n",
    "# MM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# f2 = lambda x: x * 0.85 + 0.15 / 5\n",
    "# MM = f2(MM)\n",
    "# MM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "from NLP import nlpfunctions\n",
    "import json\n",
    "# \n",
    "# # Попробовать с добивкой символов и без лемматизации\n",
    "# \n",
    "# texts = nlpfunctions.eat_json('dataset_43428_1.txt')\n",
    "# with open('output', 'w', encoding='UTF-8') as file:\n",
    "#     res = []\n",
    "#     i = 0\n",
    "#     for text in texts:\n",
    "#         i += 1\n",
    "#         \n",
    "#         M = np.array(nlpfunctions.sentence_to_matrix(text), dtype=float)\n",
    "#         vec = np.zeros(len(M))\n",
    "#         vec[0] = 1\n",
    "#         \n",
    "#         for _ in range(100):\n",
    "#             vec_n = vec.dot(M).transpose()\n",
    "#             if ((vec_n - vec) ** 2).any() < 0.000001:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 vec = vec_n\n",
    "#                 \n",
    "#         n = 300 - len(text[0])\n",
    "#         sen = [text[0]]\n",
    "#         vec[0] = 0\n",
    "#         \n",
    "#         maxi = np.argmax(vec)\n",
    "#         while n > 0:\n",
    "#             vec[maxi] = 0\n",
    "#             sen.append(text[maxi])\n",
    "#             n = n - len(text[maxi])\n",
    "#             maxi = np.argmax(vec)\n",
    "#         \n",
    "#         res.append(''.join(sen))\n",
    "#         print('ping' + str(i))\n",
    "#     json.dump(res, file, ensure_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# texts = nlpfunctions.eat_json('dataset_43428_1.txt')\n",
    "# with open('output', 'w', encoding='UTF-8') as file:\n",
    "#     res = []\n",
    "#     for text in texts:\n",
    "#         n = 300 - len(text[0])\n",
    "#         sen = [text[0]]\n",
    "#         i = 1\n",
    "#         while n > 0 and i < len(text):\n",
    "#             n = n - len(text[i])\n",
    "#             sen.append(text[i])\n",
    "#             i += 1\n",
    "#         res.append(''.join(sen))\n",
    "#     json.dump(res, file, ensure_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# Попробовать еще:\n",
    "\n",
    "# 3) Добавить доп. параметры\n",
    "# 5) биграмы, триграмы\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, encoding=\"UTF-8\") as file:\n",
    "        return file.read().split(\"\\n\")\n",
    "\n",
    "dataset = read_file('data\\\\texts_train.txt')[0:-1]\n",
    "y = read_file(\"data\\\\scores_train.txt\")[0:-1]\n",
    "normalized_dataset = read_file('data\\\\normalized_dataset')[0:-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "importance_dictionary = nlpfunctions.expression_scores_bigr(normalized_dataset, y, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "[('пора', 'перечитывать'),\n ('раз', 'просто'),\n ('у', 'ребенок'),\n ('трилогия', 'я'),\n ('не_мочь', 'оставаться'),\n ('супер', 'супер'),\n ('и', 'лирический'),\n ('изумительный', 'книга'),\n ('отличный', 'перевод'),\n ('просто', 'гений')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 55
    }
   ],
   "source": [
    "items = importance_dictionary.items()\n",
    "important_words = [word for word, _ in sorted(items, key=lambda x: x[1], reverse=True)][0:500]\n",
    "important_words[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "'Start learning'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 56
    }
   ],
   "source": [
    "X = nlpfunctions.prepare_bow(normalized_dataset, important_words)\n",
    "for x, text in zip(X, dataset):\n",
    "    x.append(nlpfunctions.excl_feature(text))\n",
    "    x.append(nlpfunctions.open_brackets_feature(text))\n",
    "    x.append(nlpfunctions.close_brackets_feature(text))\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, random_state=0)\n",
    "'Start learning'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_eval = np.array(y_eval, dtype=np.int8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\kafree\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "8.853"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 58
    }
   ],
   "source": [
    "model = LinearSVC(max_iter=20000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = np.array(model.predict(X_eval), dtype=np.int8)\n",
    "mean_squared_error(y_eval, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# test_texts = read_file('data\\\\dataset_40757_1.txt')[0:-1]\n",
    "# test_dataset = nlpfunctions.test_texts_normalize(test_texts)\n",
    "# X_test = nlpfunctions.prepare_bow(test_dataset, important_words)\n",
    "# for x, text in zip(X_test, test_texts):\n",
    "#     x.append(nlpfunctions.excl_feature(text))\n",
    "#     x.append(nlpfunctions.open_brackets_feature(text))\n",
    "#     x.append(nlpfunctions.close_brackets_feature(text))\n",
    "# "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_test)\n",
    "# with open('data\\\\answer.txt', 'w', encoding='UTF-8') as file:\n",
    "#     for pred in y_pred:\n",
    "#         file.write(str(pred) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
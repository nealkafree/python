{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# from nltk.tag.perceptron import PerceptronTagger\n",
    "# \n",
    "# corpus = postagging.eat_corpus(postagging.CORPUS)\n",
    "# k = round(len(corpus) * 0.8)\n",
    "# train_corpus = corpus[0:k]\n",
    "# test_corpus = corpus[k:]\n",
    "# perc_tagger = PerceptronTagger(load=False)\n",
    "# perc_tagger.train(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# n = 0\n",
    "# right = 0\n",
    "# for sentence in test_corpus:\n",
    "#     x = [word[0] for word in sentence]\n",
    "#     y = [word[1] for word in sentence]\n",
    "#     y_pred = [word[1] for word in perc_tagger.tag(x)]\n",
    "#     n += len(sentence)\n",
    "#     for tag_real, tag_pred in zip(y, y_pred):\n",
    "#         if tag_real == tag_pred:\n",
    "#             right += 1\n",
    "# right / n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "# \n",
    "# hmm_tagger = HiddenMarkovModelTagger.train(train_corpus, test_sequence=test_corpus)\n",
    "# "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# for sentence in test_corpus:\n",
    "#     x = [word[0] for word in sentence]\n",
    "#     y = [word[1] for word in sentence]\n",
    "#     y_pred = [word[1] for word in hmm_tagger.tag(x)]\n",
    "#     n += len(sentence)\n",
    "#     for tag_real, tag_pred in zip(y, y_pred):\n",
    "#         if tag_real == tag_pred:\n",
    "#             right += 1\n",
    "# right / n\n",
    "# "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# dictionary = postagging.eat_dictionary(postagging.DICTIONARY)\n",
    "# "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# text = []\n",
    "# with open('dataset_37845_1.txt', encoding='UTF-8') as file:\n",
    "#     for line in file.readlines():\n",
    "#         text.append(postagging.prepare_sentence(line))\n",
    "# \n",
    "# with open('output', 'w', encoding='UTF-8') as file:\n",
    "#     for sentence in text:\n",
    "#         res = hmm_tagger.tag([word.lower() for word in sentence])\n",
    "#         output = []\n",
    "#         for word, tag in zip(sentence, res):\n",
    "#             if '_'.join(tag) in dictionary:\n",
    "#                 lemma = dictionary['_'.join(tag)]\n",
    "#             else:\n",
    "#                 lemma = tag[0]\n",
    "#             output.append(word + '{' + lemma + '=' + tag[1] + '}')\n",
    "#         file.write(' '.join(output) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# M = np.array([[0, 1, 0, 1, 1],[1, 1, 1, 1, 1],[0, 1, 0, 0, 0],[0, 0, 1, 0, 1],[1, 0, 0, 0, 0]], dtype=float)\n",
    "# ones = np.ones((5,5))\n",
    "# MM = M.dot(ones) * M"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# f = lambda x: 1 / x\n",
    "# MM = f(MM) * M\n",
    "# MM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# where = np.isnan(MM)\n",
    "# MM[where] = 0\n",
    "# MM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# f2 = lambda x: x * 0.85 + 0.15 / 5\n",
    "# MM = f2(MM)\n",
    "# MM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from NLP import nlpfunctions\n",
    "import json\n",
    "# \n",
    "# # Попробовать с добивкой символов и без лемматизации\n",
    "# \n",
    "# texts = nlpfunctions.eat_json('dataset_43428_1.txt')\n",
    "# with open('output', 'w', encoding='UTF-8') as file:\n",
    "#     res = []\n",
    "#     i = 0\n",
    "#     for text in texts:\n",
    "#         i += 1\n",
    "#         \n",
    "#         M = np.array(nlpfunctions.sentence_to_matrix(text), dtype=float)\n",
    "#         vec = np.zeros(len(M))\n",
    "#         vec[0] = 1\n",
    "#         \n",
    "#         for _ in range(100):\n",
    "#             vec_n = vec.dot(M).transpose()\n",
    "#             if ((vec_n - vec) ** 2).any() < 0.000001:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 vec = vec_n\n",
    "#                 \n",
    "#         n = 300 - len(text[0])\n",
    "#         sen = [text[0]]\n",
    "#         vec[0] = 0\n",
    "#         \n",
    "#         maxi = np.argmax(vec)\n",
    "#         while n > 0:\n",
    "#             vec[maxi] = 0\n",
    "#             sen.append(text[maxi])\n",
    "#             n = n - len(text[maxi])\n",
    "#             maxi = np.argmax(vec)\n",
    "#         \n",
    "#         res.append(''.join(sen))\n",
    "#         print('ping' + str(i))\n",
    "#     json.dump(res, file, ensure_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# texts = nlpfunctions.eat_json('dataset_43428_1.txt')\n",
    "# with open('output', 'w', encoding='UTF-8') as file:\n",
    "#     res = []\n",
    "#     for text in texts:\n",
    "#         n = 300 - len(text[0])\n",
    "#         sen = [text[0]]\n",
    "#         i = 1\n",
    "#         while n > 0 and i < len(text):\n",
    "#             n = n - len(text[i])\n",
    "#             sen.append(text[i])\n",
    "#             i += 1\n",
    "#         res.append(''.join(sen))\n",
    "#     json.dump(res, file, ensure_ascii=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}